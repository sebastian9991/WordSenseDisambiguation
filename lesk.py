from loader import load_instances, load_key
from embeddings_nn import call_train, call_predict
import nltk
import math
import json
from nltk.corpus import wordnet as wn
from nltk.corpus import semcor
from nltk import Tree
from nltk.wsd import lesk
from nltk.corpus import stopwords
from collections import defaultdict
from tabulate import tabulate
import matplotlib.pyplot as plt

'''
Convert the wordnet keys to Synets. We assume this is the correct label
FORM: group%1:03:00:: 
'''
def key_to_synets(word_key):
    return wn.synset_from_sense_key(word_key)
'''
Sense indicates as #1 in the synset according to WordNet
'''
def most_frequent_baseline(lemma, pos = None): 
    synsets = wn.synsets(lemma, pos = pos)
    #Return the first synset
    return synsets[0] if synsets else None

'''
NLTK implementation of Lesk's WSD
'''
def lesk_wsd(context, lemma, pos = None):
    return lesk(context_sentence=context, ambiguous_word= lemma
                , pos=pos)

'''
Method A.
Starter code from: https://www.nltk.org/_modules/nltk/wsd.html#lesk
This method refers from the Corpus Lesk algorithm in the textbook; Section 20.4 (Jurafsky, D. 2nd)
We add two things to the original Lesk algorithm: 
1. Increase the signature set size by including more sentences of each sense. (e.g. from SemCor)
2. Use Inverse Document Frequency to weight each overlapping word
'''
def corpus_lesk(context_sentence, ambiguous_word, pos=None, synsets=None, lang="eng"):
    """Return a synset for an ambiguous word in a context.
    """

    context = set(context_sentence)
    synset_sentence_index = load_synset_sentence_index()

    if synsets is None:
        synsets = wn.synsets(ambiguous_word, lang=lang)

    if pos:
        synsets = [ss for ss in synsets if str(ss.pos()) == pos]

    if not synsets:
        return None

    '''
    Get the sentences for the synset from the index
    '''
    def get_sentences_for_synset(synset):
        synset_name = synset.name()
        try:
            return synset_sentence_index[synset_name]
        except:
            return []
            
    """
    Calculate the weighted overlap by summing the idf scores of intersecting words
    """
    def idf_weighted_overlap(synset, idf_scores):
        signature = set((synset.definition() + ' '.join(get_sentences_for_synset(synset))).split())
        return sum(idf_scores.get(word, 0) for word in context.intersection(signature))

    """
    Calculate the IDF scores for words across the SemCore sentences, also returns sentences for efficiency
    (We do not want to search SemCor twice)
    """
    def calculate_idf_scores_get_sentences(synset):
        word_document_count = defaultdict(int)
        Ndoc = 0

        
        sentences = get_sentences_for_synset(synset)
        if len(sentences) > 1:
            Ndoc += len(sentences)
        else: 
            #For the definition document
            Ndoc = 1
        
        for sentence in sentences:
            words = set(sentence.split())
            for word in words: 
                word_document_count[word] += 1
        for word in synset.definition():
            word_document_count[word] += 1
            
        
        idf_scores = {}
        for word, count in word_document_count.items():
            if count > 0:
                idf_scores[word] = math.log(Ndoc / count)
            else: 
                idf_scores[word] = 0
                
            #print(f"Word: {word}, Ndoc: {Ndoc}, Count: {count}, IDF: {idf_scores[word]}") 

        
        return idf_scores
     
    _, sense = max((idf_weighted_overlap(ss, calculate_idf_scores_get_sentences(ss)), ss) for ss in synsets)

    return sense


'''
Load the synset index from json file generated by index.py
'''
def load_synset_sentence_index():
    """Load the preprocessed Synset to sentences index from a file."""
    with open("synset_to_sentences_index.json", "r") as file:
        synset_to_sentences = json.load(file)
    return synset_to_sentences
    
'''
We use SemCor a subset of the Brown corpus tagged senses to produce corpus
added to the signature of each sense. 
NOTE: This was not used for efficiencies sake, alas it is good to leave to display why we indexed a JSON of the semcor prior in index.py
'''
def get_semcor_sentences_for_synset(synset): 
    sentences = []
    
    for sent_tagged, sent in zip(semcor.tagged_sents(tag='both'), semcor.sents()): 
        for chunk in sent_tagged:
            if isinstance(chunk, Tree): 
                if hasattr(chunk.label(), 'synset'):
                    chunk_synset = chunk.label().synset()
                    if chunk_synset.path_similarity(synset) == 1:
                        #Then they are equal
                        single_string = ' '.join(w for w in sent)
                        sentences.append(single_string)
                        break
        if len(sentences) >= 1: 
            #We limit the extra corpus to 1 sentences for efficency.
            break
    
    return sentences

'''
Checks the equality of two Synsets. 
We employ the use of wn.path_similarity() returns [0, 1]
If they are equal we have 1, otherwise ranges between [0, 1)
'''
def score_synset_lists(syn_1_l, syn_2_l):
    sum = 0
    for syn_1, syn_2 in zip(syn_1_l, syn_2_l):
        sum += syn_1.path_similarity(syn_2)

    assert(len(syn_1_l) == len(syn_1_l))
    return sum / len(syn_1_l)



'''
Download some nltk tools and get stopwords
'''
def get_nltk_tools():
    #Create tools
    stop_words = stopwords.words('english')
    return stop_words

'''
Preprocess the context sentence, namely remove the stop words
'''
def preprocess_context_sentence(context_list):
    stop_words = get_nltk_tools()
    stop_words = set(stop_words)
    
    context_list_preprocessed = []
    for w in context_list:
        if w not in stop_words:
            context_list_preprocessed.append(w)
    
    return context_list_preprocessed

'''
Maps the Penn Treebank Tags to given wn tags
'''
def pos_tag_mapper(tag):
    if tag in {"NN", "NNS", "NNP", "NNPS"}:
        return wn.NOUN
    elif tag in {"VB", "VBD", "VBG", "VBN", "VBP", "VBZ"}:
        return wn.VERB
    elif tag in {"JJ", "JJR", "JJS"}:
        return wn.ADJ
    elif tag in {"RB", "RBR", "RBS"}:
        return wn.ADV
    return None

'''
Process a dictonary that is better structured for my methods, and refactoring
'''
def process_data(keys, instances):
    data = []
    for item, key in zip(instances.items(), keys):
        data_element = {"word": "", "context": None, "pos": None, "label": None, "incorrect_label": None}
        syn_key = keys[key][0]
        y_label = key_to_synets(syn_key)
        wsd_instance = item[1]
        context = list(map(lambda x: x.decode("utf-8"), wsd_instance.context))
        lemma = wsd_instance.lemma.decode("utf-8")
        pos = wsd_instance.pos.decode("utf-8")
        #Get incorrect label
        synsets = wn.synsets(lemma)
        for sense in synsets:
            if sense.path_similarity(y_label) != 1:
                data_element['incorrect_label'] = sense
                break
        if data_element['incorrect_label'] is None:
            print("No Incorrect Label found")
            data_element['incorrect_label'] = y_label

        
        data_element['word'] = lemma
        data_element['context'] = context    
        data_element['pos'] = pos
        data_element['label'] = y_label
        data.append(data_element)
    
    return data



def method_calls(data, title_string):

    display_dict = {
        "most_frequent_baseline": 0,
        "lesk_score_raw": 0,
        "lesk_score_pos" : 0,
        "lesk_score_preprocessed": 0, 
        "lesk_score_preprocessed_pos": 0, 
        "corpus_lesk": 0, 
        "corpus_lesk_pos": 0, 
        "FFN, BERT-Embeddings" :0, 
        "FFN, BERT-Embeddings, incorrect-labels": 0,
    }

    y_correct_labels = []
    x_most_frequent_baseline = []
    x_lesk_syn_predictions = []
    x_lesk_syn_predictions_pos = []
    x_lesk_syn_predictions_pre = []
    x_lesk_syn_predictions_pre_pos = []
    x_corpus_lesk = [] 
    x_corpus_lesk_pos = [] 

    for element in data:
        #Get label:
        y_correct_labels.append(element['label'])
        #Baseline:
        x_most_frequent_baseline.append(most_frequent_baseline(element['word']))
        #Lesk:
        x_lesk_syn_predictions.append(lesk_wsd(element['context'], element['word']))
        x_lesk_syn_predictions_pos.append(lesk_wsd(element['context'], element['word'], pos = pos_tag_mapper(element['pos'])))
        
        ##After preprocessing
        context_pre = preprocess_context_sentence(context_list=element['context'])
        x_lesk_syn_predictions_pre.append(lesk_wsd(context_pre, element['word']))
        x_lesk_syn_predictions_pre_pos.append(lesk_wsd(context=context_pre, lemma = element['word'], pos=pos_tag_mapper(element['pos'])))

        #My Methods:
        #METHOD A: Corpus Lesk
        x_corpus_lesk.append(corpus_lesk(element['context'], element['word']))
        x_corpus_lesk_pos.append(corpus_lesk(element['context'], element['word'], pos= pos_tag_mapper(element['pos'])))


    #Method B: FFN with Bert embeddings
    #Train the NN first
    if title_string == "Dev":
        print("Training NN called")
        call_train(data)




    
    display_dict['most_frequent_baseline'] = score_synset_lists(y_correct_labels, x_most_frequent_baseline)
    display_dict['lesk_score_raw'] = score_synset_lists(y_correct_labels, x_lesk_syn_predictions)
    display_dict['lesk_score_pos'] = score_synset_lists(y_correct_labels, x_lesk_syn_predictions_pos)
    display_dict['lesk_score_preprocessed'] = score_synset_lists(y_correct_labels, x_lesk_syn_predictions_pre)
    display_dict["lesk_score_preprocessed_pos"] = score_synset_lists(y_correct_labels, x_lesk_syn_predictions_pre_pos)
    display_dict["corpus_lesk"] = score_synset_lists(y_correct_labels, x_corpus_lesk)
    display_dict["corpus_lesk_pos"] = score_synset_lists(y_correct_labels, x_corpus_lesk_pos)
    display_dict["FFN, BERT-Embeddings"] = call_predict(data)[0]
    display_dict["FFN, BERT-Embeddings, incorrect-labels"] = call_predict(data)[1]
    
    table = tabulate(display_dict.items(), headers=["Method", title_string + " Score"], tablefmt="pretty")
    print(table)

    #for plots
    methods = list(display_dict.keys()) 
    scores = list(display_dict.values()) 

    plt.figure(figsize=(10, 6))
    plt.bar(methods, scores, color='skyblue')
    plt.ylabel('Scores')
    plt.xlabel('Methods')
    plt.title(f'{title_string} - Score Distribution')
    plt.ylim(0, 1)  
    plt.xticks(rotation=45, ha='right') 
    plt.tight_layout()

    plt.show()

    
'''
used from @jcheung start code
Main method
'''
def main():
    nltk.download('stopwords')
    nltk.download('semcor')
    data_f = 'multilingual-all-words.en.xml'
    key_f = 'wordnet.en.key'
    dev_instances, test_instances = load_instances(data_f)
    dev_key, test_key = load_key(key_f)
    
    # IMPORTANT: keys contain fewer entries than the instances; need to remove them
    dev_instances = {k:v for (k,v) in dev_instances.items() if k in dev_key}
    test_instances = {k:v for (k,v) in test_instances.items() if k in test_key}

    data_dev = process_data(dev_key, dev_instances)
    method_calls(data_dev, "Dev")
    
    data_test = process_data(test_key, test_instances)
    method_calls(data_test, "Test")



    ##Example output
    #We define two sentences to observe the output of Corpus Lesk
    # The plant was thriving despite the harsh winter conditions.
    #Do we mean plant as in  plant.n.01 or plant.n.02
    context = ['The', 'plant', 'was', 'thriving', 'despite', 'the', 'harsh', 'winter', 'conditions']
    word = 'plant'
    sense = wn.synset('plant.n.01')
    syn_corpus_lesk = corpus_lesk(context_sentence=context, ambiguous_word=word)
    print("Synset corpus lesk output:")
    print(syn_corpus_lesk) 




if __name__ == '__main__':
    main()